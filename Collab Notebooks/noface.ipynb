{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1oQz612-KOfi8uOC4yG0wRKvCozDO7hjr","authorship_tag":"ABX9TyNQrTvEF1pIC5+TVG6asmc2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","import numpy as np\n","\n","num_classes = 3\n","emotion_classes = ['No Face', 'Multiple Face','Blurred Image']\n","\n","batch_size = 16\n","epochs = 9\n","input_shape = (224, 224, 3)\n","learning_rate = 0.0001  # Reduced learning rate for potentially better convergence\n","\n","model = keras.Sequential([\n","    Conv2D(64, (3, 3), activation='relu', input_shape=input_shape),\n","    MaxPooling2D((2, 2)),\n","    Conv2D(128, (3, 3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Conv2D(256, (3, 3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Flatten(),\n","    Dense(512, activation='relu'),  # Increased the number of units\n","    Dropout(0.5),\n","    Dense(num_classes, activation='softmax')\n","])\n","\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","train_datagen = ImageDataGenerator(\n","    rescale=1.0/255,\n","    rotation_range=30,  # Increased rotation range\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,  # Added shear range\n","    zoom_range=0.2,  # Added zoom range\n","    horizontal_flip=True,\n","    fill_mode='nearest')\n","\n","test_datagen = ImageDataGenerator(rescale=1.0/255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    '/content/drive/MyDrive/Datasets/Face_No_Face_Multiple_face_blurred_dataset/train',\n","    target_size=input_shape[:2],\n","    batch_size=batch_size,\n","    class_mode='categorical'\n",")\n","\n","test_generator = test_datagen.flow_from_directory(\n","    '/content/drive/MyDrive/Datasets/Face_No_Face_Multiple_face_blurred_dataset/test',\n","    target_size=input_shape[:2],\n","    batch_size=batch_size,\n","    class_mode='categorical'\n",")\n","\n","model.fit(\n","    train_generator,\n","    epochs=epochs,\n","    verbose=1,\n","    validation_data=test_generator\n",")\n","\n","save_path = '/content/drive/MyDrive/Datasets1/noface_final.keras'\n","model.save(save_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EZcNG6YCsK8E","executionInfo":{"status":"ok","timestamp":1702819785874,"user_tz":-330,"elapsed":1838933,"user":{"displayName":"SIH2023","userId":"13302485182683912673"}},"outputId":"5649d779-aedb-4b43-ff9b-d71ab1715e73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 321 images belonging to 3 classes.\n","Found 193 images belonging to 3 classes.\n","Epoch 1/9\n","21/21 [==============================] - 250s 11s/step - loss: 0.9140 - accuracy: 0.6604 - val_loss: 0.8998 - val_accuracy: 0.6580\n","Epoch 2/9\n","21/21 [==============================] - 153s 7s/step - loss: 0.8369 - accuracy: 0.6791 - val_loss: 0.7970 - val_accuracy: 0.6580\n","Epoch 3/9\n","21/21 [==============================] - 176s 8s/step - loss: 0.8075 - accuracy: 0.6822 - val_loss: 0.7515 - val_accuracy: 0.6528\n","Epoch 4/9\n","21/21 [==============================] - 154s 7s/step - loss: 0.7490 - accuracy: 0.6978 - val_loss: 0.7360 - val_accuracy: 0.6736\n","Epoch 5/9\n","21/21 [==============================] - 153s 7s/step - loss: 0.7365 - accuracy: 0.7072 - val_loss: 0.7083 - val_accuracy: 0.6373\n","Epoch 6/9\n","21/21 [==============================] - 172s 8s/step - loss: 0.7197 - accuracy: 0.7009 - val_loss: 0.6976 - val_accuracy: 0.6580\n","Epoch 7/9\n","21/21 [==============================] - 186s 9s/step - loss: 0.7153 - accuracy: 0.7040 - val_loss: 0.8888 - val_accuracy: 0.6736\n","Epoch 8/9\n","21/21 [==============================] - 159s 8s/step - loss: 0.6539 - accuracy: 0.7414 - val_loss: 0.6751 - val_accuracy: 0.6788\n","Epoch 9/9\n","21/21 [==============================] - 186s 9s/step - loss: 0.6511 - accuracy: 0.7321 - val_loss: 0.6786 - val_accuracy: 0.6839\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1QGI8UAJ3gXj","executionInfo":{"status":"ok","timestamp":1703069784166,"user_tz":-330,"elapsed":14961,"user":{"displayName":"SIH2023","userId":"13302485182683912673"}},"outputId":"6c66cb11-8807-41be-c83e-9d172cb7f3b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["save_path = '/content/drive/MyDrive/Datasets1/noface_final.h5'\n","model.save(save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6h2oaMnisLFW","executionInfo":{"status":"ok","timestamp":1702819807204,"user_tz":-330,"elapsed":11356,"user":{"displayName":"SIH2023","userId":"13302485182683912673"}},"outputId":"10b1cd7e-a543-4a27-81fd-fc4bdeeef24b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}]},{"cell_type":"code","source":["save_path = '/content/drive/MyDrive/models/noface_final.h5'\n","model.save(save_path)"],"metadata":{"id":"B6_igMbd-D-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_image_path = '/content/drive/MyDrive/Datasets/Face_No_Face_Multiple_face_blurred_dataset/validation/blurred/WhatsApp Image 2023-12-17 at 2.10.58 PM.jpeg'\n","img = keras.preprocessing.image.load_img(example_image_path, target_size=input_shape[:2])\n","img = keras.preprocessing.image.img_to_array(img)\n","img = np.expand_dims(img, axis=0)\n","img /= 255.0\n","\n","\n","predictions = model.predict(img)\n","\n","for i, emotion in enumerate(emotion_classes):\n","    print(f'Predicted {emotion}: {predictions[0][i] * 100:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TN-bDDW7vDo8","executionInfo":{"status":"ok","timestamp":1702820002062,"user_tz":-330,"elapsed":384,"user":{"displayName":"SIH2023","userId":"13302485182683912673"}},"outputId":"79514eb4-fdc8-4aa4-bd93-e44c8e27e7ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 138ms/step\n","Predicted No Face: 4.45%\n","Predicted Multiple Face: 26.52%\n","Predicted Blurred Image: 69.04%\n"]}]},{"cell_type":"code","source":["example_image_path = '/content/drive/MyDrive/Datasets/Face_No_Face_Multiple_face_blurred_dataset/validation/Multiple_face/WhatsApp Image 2023-12-16 at 12.59.39 PM.jpeg'\n","img = keras.preprocessing.image.load_img(example_image_path, target_size=input_shape[:2])\n","img = keras.preprocessing.image.img_to_array(img)\n","img = np.expand_dims(img, axis=0)\n","img /= 255.0\n","\n","\n","predictions = model.predict(img)\n","\n","for i, emotion in enumerate(emotion_classes):\n","    print(f'Predicted {emotion}: {predictions[0][i] * 100:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3uKtQK9_PFz","executionInfo":{"status":"ok","timestamp":1702820166522,"user_tz":-330,"elapsed":640,"user":{"displayName":"SIH2023","userId":"13302485182683912673"}},"outputId":"7f8418f6-90ff-4e8f-dcee-3545259c9718"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 216ms/step\n","Predicted No Face: 27.61%\n","Predicted Multiple Face: 71.39%\n","Predicted Blurred Image: 1.00%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import joblib"],"metadata":{"id":"VHTJS7UTXl4d","executionInfo":{"status":"ok","timestamp":1706182063122,"user_tz":-330,"elapsed":3164,"user":{"displayName":"SIH2023","userId":"13302485182683912673"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["noface_model = keras.models.load_model('/content/drive/MyDrive/models/noface_final.h5')"],"metadata":{"id":"fOa3KNjEXmwr","executionInfo":{"status":"ok","timestamp":1706182135115,"user_tz":-330,"elapsed":17073,"user":{"displayName":"SIH2023","userId":"13302485182683912673"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["example_image_path = '/content/drive/MyDrive/Datasets/Face_No_Face_Multiple_face_blurred_dataset/validation/blurred/WhatsApp Image 2023-12-17 at 2.10.58 PM.jpeg'\n","input_shape = (224, 224, 3)\n","num_classes = 3\n","emotion_classes = ['No Face', 'Multiple Face','Blurred Image']\n","img = keras.preprocessing.image.load_img(example_image_path, target_size=input_shape[:2])\n","img = keras.preprocessing.image.img_to_array(img)\n","img = np.expand_dims(img, axis=0)\n","img /= 255.0\n","\n","\n","predictions = noface_model.predict(img)\n","\n","for i, emotion in enumerate(emotion_classes):\n","    print(f'Predicted {emotion}: {predictions[0][i] * 100:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KIs5rGyEX3bz","executionInfo":{"status":"ok","timestamp":1706182204700,"user_tz":-330,"elapsed":705,"user":{"displayName":"SIH2023","userId":"13302485182683912673"}},"outputId":"ff732bed-d15a-4503-855e-d4bee6c78833"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 132ms/step\n","Predicted No Face: 4.45%\n","Predicted Multiple Face: 26.52%\n","Predicted Blurred Image: 69.04%\n"]}]},{"cell_type":"code","source":["example_image_path = '/content/drive/MyDrive/Datasets/Face_No_Face_Multiple_face_blurred_dataset/validation/Multiple_face/WhatsApp Image 2023-12-16 at 12.59.39 PM.jpeg'\n","img = keras.preprocessing.image.load_img(example_image_path, target_size=input_shape[:2])\n","img = keras.preprocessing.image.img_to_array(img)\n","img = np.expand_dims(img, axis=0)\n","img /= 255.0\n","\n","\n","predictions = noface_model.predict(img)\n","\n","for i, emotion in enumerate(emotion_classes):\n","    print(f'Predicted {emotion}: {predictions[0][i] * 100:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nl1Adz72YZ1k","executionInfo":{"status":"ok","timestamp":1706182232097,"user_tz":-330,"elapsed":9,"user":{"displayName":"SIH2023","userId":"13302485182683912673"}},"outputId":"507d094f-9b06-4625-eeb7-5ba5d482eefa"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 138ms/step\n","Predicted No Face: 27.61%\n","Predicted Multiple Face: 71.39%\n","Predicted Blurred Image: 1.00%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"uzXNFwsfYqUe"},"execution_count":null,"outputs":[]}]}