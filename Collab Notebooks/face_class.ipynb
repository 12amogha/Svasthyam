{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1l7t063kCpEMX0LUToywLeVrKtqljdWIy","authorship_tag":"ABX9TyNEsBzPb+70svw/SHnVglTs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uT2mnNMRNGrE","executionInfo":{"status":"ok","timestamp":1702670477613,"user_tz":-330,"elapsed":13612,"user":{"displayName":"SIH2023","userId":"13302485182683912673"}},"outputId":"d000a99d-8b7b-4199-b77f-ca652084ecdb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Import necessary libraries\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","import numpy as np\n","\n","# Define the emotion classes\n","num_classes = 3\n","emotion_classes = ['Anxiety', 'Sadness', 'Happiness']\n","\n","# Set hyperparameters\n","batch_size = 32\n","epochs = 7\n","input_shape = (224, 224, 3)  # Input image size (adjust as needed)\n","learning_rate = 0.001\n","\n","# Define a CNN model\n","model = keras.Sequential([\n","    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n","    MaxPooling2D((2, 2)),\n","    Conv2D(64, (3, 3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Conv2D(128, (3, 3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dropout(0.5),\n","    Dense(num_classes, activation='softmax')\n","])\n","\n","# Compile the model\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Data augmentation and preprocessing for training\n","train_datagen = ImageDataGenerator(\n","    rescale=1.0/255,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest')\n","\n","# Data augmentation for testing (only rescaling)\n","test_datagen = ImageDataGenerator(rescale=1.0/255)\n","\n","# Load training data\n","train_generator = train_datagen.flow_from_directory(\n","    '/content/drive/MyDrive/Datasets/Face_Dataset/train',\n","    target_size=input_shape[:2],\n","    batch_size=batch_size,\n","    class_mode='categorical'\n",")\n","\n","# Load testing data\n","test_generator = test_datagen.flow_from_directory(\n","    '/content/drive/MyDrive/Datasets/Face_Dataset/test',\n","    target_size=input_shape[:2],\n","    batch_size=batch_size,\n","    class_mode='categorical'\n",")\n","\n","# Train the model\n","model.fit(\n","    train_generator,\n","    epochs=epochs,\n","    verbose=1,\n","    validation_data=test_generator\n",")\n","\n","# Save the trained model\n","save_path='/content/drive/MyDrive/models/face_model.keras'\n","model.save('save_path')\n","\n","# Make predictions on an example image\n","example_image_path = '/content/drive/MyDrive/Datasets/Face_Dataset/test/Anxiety/anx11.jpg'\n","img = keras.preprocessing.image.load_img(example_image_path, target_size=input_shape[:2])\n","img = keras.preprocessing.image.img_to_array(img)\n","img = np.expand_dims(img, axis=0)\n","img /= 255.0  # Preprocess the image\n","\n","# Make predictions\n","predictions = model.predict(img)\n","\n","# Display predicted emotion percentages\n","for i, emotion in enumerate(emotion_classes):\n","    print(f'Predicted {emotion}: {predictions[0][i] * 100:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"60p8R03VNsr_","executionInfo":{"status":"ok","timestamp":1702670725160,"user_tz":-330,"elapsed":247550,"user":{"displayName":"SIH2023","userId":"13302485182683912673"}},"outputId":"d3dd7c30-83aa-4634-f04c-a8db9005f79b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 139 images belonging to 3 classes.\n","Found 39 images belonging to 3 classes.\n","Epoch 1/7\n","5/5 [==============================] - 57s 13s/step - loss: 2.0050 - accuracy: 0.3597 - val_loss: 1.1355 - val_accuracy: 0.2308\n","Epoch 2/7\n","5/5 [==============================] - 21s 4s/step - loss: 1.0839 - accuracy: 0.4173 - val_loss: 1.0161 - val_accuracy: 0.5128\n","Epoch 3/7\n","5/5 [==============================] - 22s 4s/step - loss: 1.0639 - accuracy: 0.4460 - val_loss: 1.0268 - val_accuracy: 0.5128\n","Epoch 4/7\n","5/5 [==============================] - 20s 4s/step - loss: 1.0281 - accuracy: 0.5036 - val_loss: 1.0046 - val_accuracy: 0.5128\n","Epoch 5/7\n","5/5 [==============================] - 19s 4s/step - loss: 1.0787 - accuracy: 0.4748 - val_loss: 1.0224 - val_accuracy: 0.5128\n","Epoch 6/7\n","5/5 [==============================] - 21s 4s/step - loss: 1.0495 - accuracy: 0.4964 - val_loss: 1.0549 - val_accuracy: 0.5128\n","Epoch 7/7\n","5/5 [==============================] - 21s 4s/step - loss: 1.0288 - accuracy: 0.5180 - val_loss: 0.9936 - val_accuracy: 0.5128\n","1/1 [==============================] - 0s 148ms/step\n","Predicted Anxiety: 54.95%\n","Predicted Sadness: 28.35%\n","Predicted Happiness: 16.70%\n"]}]},{"cell_type":"code","source":["# Save the trained model to the specified location\n","save_path = '/content/drive/MyDrive/models/face_model.keras'\n","model.save(save_path)"],"metadata":{"id":"6WHN9JKPNszK","executionInfo":{"status":"ok","timestamp":1702670727284,"user_tz":-330,"elapsed":2130,"user":{"displayName":"SIH2023","userId":"13302485182683912673"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","\n","# Define the emotion classes\n","num_classes = 3\n","emotion_classes = ['Anxiety', 'Sadness', 'Happiness']\n","\n","# Set input image size (adjust as needed)\n","input_shape = (224, 224, 3)\n","\n","# Load the trained model\n","model_path = '/content/drive/MyDrive/models/face_model.keras'\n","model = keras.models.load_model(model_path)\n","\n","# Specify the path to the image you want to predict\n","example_image_path = '/content/drive/MyDrive/Datasets/Face_Dataset/test/Anxiety/anx7.jpg'\n","\n","# Load and preprocess the image\n","img = keras.preprocessing.image.load_img(example_image_path, target_size=input_shape[:2])\n","img = keras.preprocessing.image.img_to_array(img)\n","img = np.expand_dims(img, axis=0)\n","img /= 255.0  # Preprocess the image\n","\n","# Make predictions\n","predictions = model.predict(img)\n","\n","# Display predicted emotion percentages\n","for i, emotion in enumerate(emotion_classes):\n","    print(f'Predicted {emotion}: {predictions[0][i] * 100:.2f}%')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZWCw_l5SNs2c","executionInfo":{"status":"ok","timestamp":1702670732835,"user_tz":-330,"elapsed":5555,"user":{"displayName":"SIH2023","userId":"13302485182683912673"}},"outputId":"1078b938-1639-4a1f-fc5e-9af61fbd4550"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 105ms/step\n","Predicted Anxiety: 53.46%\n","Predicted Sadness: 26.48%\n","Predicted Happiness: 20.05%\n"]}]}]}